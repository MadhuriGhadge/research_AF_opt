{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjmF4qXmWyXm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JO4xmVYnVBZ",
        "outputId": "7364526e-a0ce-4975-a333-7780e458da90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYg7yb76WW37"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 25                # number of epochs per run (you chose 25)\n",
        "BATCH_SIZE = 128           # default batch size for main runs\n",
        "BATCH_SIZE_SWEEPS = [32,64,128,256]   # for batch-size experiments\n",
        "LR_DEFAULT = {\"sgd\": 0.01, \"adam\": 0.001, \"adamw\": 0.001}\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RESULTS_ROOT = Path(\"results\")         # where all outputs will be saved\n",
        "NUM_WORKERS = 2 if torch.cuda.is_available() else 0\n",
        "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeu12qDDWo-4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Create folders\n",
        "(RESULTS_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "(RESULTS_ROOT / \"ann\").mkdir(exist_ok=True)\n",
        "(RESULTS_ROOT / \"cnn\").mkdir(exist_ok=True)\n",
        "(RESULTS_ROOT / \"summary\").mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGJm_yv7XWV5"
      },
      "outputs": [],
      "source": [
        "def save_plot(fig, out_path):\n",
        "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def save_metrics_csv(rows, out_path):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(out_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uKMFqHJXWJh"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(task=\"mnist\", batch_size=128):\n",
        "    if task == \"mnist\":\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        train_ds = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        test_ds  = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "        input_shape = (1,28,28)\n",
        "    elif task == \"cifar\":\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                 (0.2023, 0.1994, 0.2010)),\n",
        "        ])\n",
        "        train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "        test_ds  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "        input_shape = (3,32,32)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown task\")\n",
        "    return train_loader, test_loader, input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PUjjueIbthW"
      },
      "outputs": [],
      "source": [
        "class SimpleANN(nn.Module):\n",
        "    def __init__(self, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        acts = {\n",
        "            \"sigmoid\": nn.Sigmoid(),\n",
        "            \"relu\": nn.ReLU(),\n",
        "            \"leakyrelu\": nn.LeakyReLU(0.01),\n",
        "            \"gelu\": nn.GELU()\n",
        "        }\n",
        "        act = acts[activation.lower()]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 512), act,\n",
        "            nn.Linear(512, 256), act,\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        acts = {\n",
        "            \"sigmoid\": nn.Sigmoid(),\n",
        "            \"relu\": nn.ReLU(),\n",
        "            \"leakyrelu\": nn.LeakyReLU(0.01),\n",
        "            \"gelu\": nn.GELU()\n",
        "        }\n",
        "        act = acts[activation.lower()]\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), act,\n",
        "            nn.Conv2d(32, 64, 3, padding=1), act,\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), act,\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128*8*8, 256), act,\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a36hRIJBb0E_"
      },
      "outputs": [],
      "source": [
        "def average_grad_norm(model):\n",
        "    total_norm_sq = 0.0\n",
        "    count = 0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            norm = p.grad.detach().float().norm(2).item()\n",
        "            total_norm_sq += norm**2\n",
        "            count += 1\n",
        "    return float(total_norm_sq ** 0.5) if count>0 else 0.0\n",
        "\n",
        "def simple_dead_neurons_count(model):\n",
        "    # Proxy: count number of neurons in Linear layers whose weight-vector norm < small_thresh\n",
        "    thresh = 1e-6\n",
        "    dead = 0\n",
        "    total = 0\n",
        "    for name, p in model.named_parameters():\n",
        "        if 'weight' in name and p.dim()==2:   # linear weight shapes (out_features, in_features)\n",
        "            W = p.detach().cpu().numpy()\n",
        "            norms = np.linalg.norm(W, axis=1)\n",
        "            dead += np.sum(norms < thresh)\n",
        "            total += W.shape[0]\n",
        "    return int(dead), int(total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2tqk5qUbzik"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn, scaler=None):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    grad_norms = []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        if DEVICE.type == \"cuda\" and scaler is not None:\n",
        "            with autocast():\n",
        "                out = model(xb)\n",
        "                loss = loss_fn(out, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            # compute grad norm on unscaled grads\n",
        "            scaler.unscale_(optimizer)\n",
        "            total_norm = average_grad_norm(model)\n",
        "            grad_norms.append(total_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            loss.backward()\n",
        "            total_norm = average_grad_norm(model)\n",
        "            grad_norms.append(total_norm)\n",
        "            optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    avg_acc = correct / len(loader.dataset)\n",
        "    avg_grad = float(np.mean(grad_norms)) if len(grad_norms)>0 else 0.0\n",
        "    return avg_loss, avg_acc, avg_grad\n",
        "\n",
        "def evaluate(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            loss = loss_fn(out, yb)\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "            correct += (out.argmax(1) == yb).sum().item()\n",
        "    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnPinP26bzWp"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(name, params, lr):\n",
        "    if name == \"sgd\":\n",
        "        return optim.SGD(params, lr=lr, momentum=0.9)\n",
        "    if name == \"adam\":\n",
        "        return optim.Adam(params, lr=lr)\n",
        "    if name == \"adamw\":\n",
        "        return optim.AdamW(params, lr=lr)\n",
        "    raise ValueError(\"Unknown optimizer\")\n",
        "\n",
        "def run_grid(tasks=(\"mnist\",\"cifar\"), activations=(\"sigmoid\",\"relu\",\"leakyrelu\",\"gelu\"), optimizers_list=(\"sgd\",\"adam\",\"adamw\"),\n",
        "             epochs=EPOCHS, batch_size=BATCH_SIZE, results_root=RESULTS_ROOT):\n",
        "    summary = []\n",
        "    for task in tasks:\n",
        "        print(f\"\\n==== Starting TASK: {task.upper()} ====\")\n",
        "        train_loader, test_loader, _ = get_dataloaders(task=task, batch_size=batch_size)\n",
        "        for act, opt_name in itertools.product(activations, optimizers_list):\n",
        "            print(f\"\\n--- RUN: {task} | act={act} | opt={opt_name} ---\")\n",
        "            # model\n",
        "            model = SimpleANN(act).to(DEVICE) if task==\"mnist\" else SmallCNN(act).to(DEVICE)\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            lr = LR_DEFAULT.get(opt_name, 0.001)\n",
        "            optimizer = get_optimizer(opt_name, model.parameters(), lr)\n",
        "            scaler = GradScaler() if DEVICE.type==\"cuda\" else None\n",
        "\n",
        "            run_name = f\"{task}_{act}_{opt_name}\"\n",
        "            out_dir = Path(results_root) / task / run_name\n",
        "            out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # lists for epoch metrics\n",
        "            rows = []\n",
        "            train_losses = []; val_losses = []\n",
        "            train_accs = []; val_accs = []\n",
        "            grad_norms = []; dead_counts = []\n",
        "\n",
        "            start_time = time.time()\n",
        "            for epoch in range(1, epochs+1):\n",
        "                tl, ta, gn = train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
        "                vl, va = evaluate(model, test_loader, loss_fn)\n",
        "                dn, total_neurons = simple_dead_neurons_count(model)\n",
        "\n",
        "                train_losses.append(tl); val_losses.append(vl)\n",
        "                train_accs.append(ta); val_accs.append(va)\n",
        "                grad_norms.append(gn); dead_counts.append(dn)\n",
        "\n",
        "                rows.append({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"train_loss\": tl, \"val_loss\": vl,\n",
        "                    \"train_acc\": ta, \"val_acc\": va,\n",
        "                    \"grad_norm\": gn, \"dead_neurons\": dn\n",
        "                })\n",
        "                print(f\"Epoch {epoch}/{epochs} | train_acc {ta:.4f} val_acc {va:.4f} grad_norm {gn:.4f} dead {dn}\")\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            # Save model state dict\n",
        "            model_path = out_dir / f\"{run_name}_final.pth\"\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "            # Save CSV\n",
        "            csv_path = out_dir / f\"{run_name}_metrics.csv\"\n",
        "            save_metrics_csv(rows, csv_path)\n",
        "\n",
        "            # Save summary row\n",
        "            summary_row = {\n",
        "                \"task\": task, \"activation\": act, \"optimizer\": opt_name,\n",
        "                \"final_train_acc\": train_accs[-1], \"final_val_acc\": val_accs[-1],\n",
        "                \"avg_grad_norm\": float(np.mean(grad_norms)), \"dead_neurons_final\": int(dead_counts[-1]),\n",
        "                \"run_time_sec\": int(elapsed), \"epochs\": epochs, \"batch_size\": batch_size, \"lr\": lr\n",
        "            }\n",
        "            summary.append(summary_row)\n",
        "            # Save summary CSV per run\n",
        "            with open(out_dir / f\"{run_name}_summary.json\",\"w\") as f:\n",
        "                json.dump(summary_row, f, indent=2)\n",
        "\n",
        "            # Save plots: accuracy, loss, gradnorm, dead neurons\n",
        "            # Accuracy\n",
        "            fig = plt.figure(figsize=(6,4))\n",
        "            plt.plot(range(1,epochs+1), train_accs, label=\"train_acc\")\n",
        "            plt.plot(range(1,epochs+1), val_accs, label=\"val_acc\")\n",
        "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{run_name} Accuracy\")\n",
        "            plt.legend(); plt.grid(True)\n",
        "            save_plot(fig, out_dir / f\"{run_name}_accuracy.png\")\n",
        "\n",
        "            # Loss\n",
        "            fig = plt.figure(figsize=(6,4))\n",
        "            plt.plot(range(1,epochs+1), train_losses, label=\"train_loss\")\n",
        "            plt.plot(range(1,epochs+1), val_losses, label=\"val_loss\")\n",
        "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(f\"{run_name} Loss\")\n",
        "            plt.legend(); plt.grid(True)\n",
        "            save_plot(fig, out_dir / f\"{run_name}_loss.png\")\n",
        "\n",
        "            # Grad norm\n",
        "            fig = plt.figure(figsize=(6,4))\n",
        "            plt.plot(range(1,epochs+1), grad_norms, marker='o')\n",
        "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Avg Grad Norm\"); plt.title(f\"{run_name} GradNorm\")\n",
        "            plt.grid(True)\n",
        "            save_plot(fig, out_dir / f\"{run_name}_gradnorm.png\")\n",
        "\n",
        "            # Dead neuron\n",
        "            fig = plt.figure(figsize=(6,4))\n",
        "            plt.plot(range(1,epochs+1), dead_counts, marker='o')\n",
        "            plt.xlabel(\"Epoch\"); plt.ylabel(\"Dead Neuron Count\"); plt.title(f\"{run_name} DeadNeurons\")\n",
        "            plt.grid(True)\n",
        "            save_plot(fig, out_dir / f\"{run_name}_deadneurons.png\")\n",
        "\n",
        "            # Save final CSV summary of the run metrics\n",
        "            # (already saved per-epoch CSV)\n",
        "            print(f\"Saved run artifacts to: {out_dir.resolve()}\")\n",
        "\n",
        "    # save global summary CSV\n",
        "    summary_csv = Path(results_root) / \"summary\" / \"all_runs_summary.csv\"\n",
        "    import pandas as pd\n",
        "    pd.DataFrame(summary).to_csv(summary_csv, index=False)\n",
        "    print(\"All experiments complete. Summary at:\", summary_csv)\n",
        "\n",
        "# -------------------------\n",
        "# Optional helpers: LR sweep & batch-size sweep\n",
        "# -------------------------\n",
        "def lr_sweep(task, activation, optimizer_name, lrs=[1e-1,1e-2,1e-3,1e-4], epochs=12, batch_size=128):\n",
        "    rows = []\n",
        "    for lr in lrs:\n",
        "        print(f\"LR sweep: {task} {activation} {optimizer_name} lr={lr}\")\n",
        "        train_loader, test_loader, _ = get_dataloaders(task=task, batch_size=batch_size)\n",
        "        model = SimpleANN(activation).to(DEVICE) if task==\"mnist\" else SmallCNN(activation).to(DEVICE)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n",
        "        scaler = GradScaler() if DEVICE.type==\"cuda\" else None\n",
        "        for epoch in range(1, epochs+1):\n",
        "            tl, ta, gn = train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
        "            vl, va = evaluate(model, test_loader, loss_fn)\n",
        "            print(f\"  epoch {epoch}/{epochs} | train_acc {ta:.3f} val_acc {va:.3f}\")\n",
        "        rows.append({\"task\":task, \"activation\":activation, \"optimizer\":optimizer_name, \"lr\":lr, \"final_val_acc\":va})\n",
        "    out = Path(RESULTS_ROOT) / \"summary\" / f\"lr_sweep_{task}_{activation}_{optimizer_name}.csv\"\n",
        "    save_metrics_csv(rows, out)\n",
        "    return out\n",
        "\n",
        "def batch_size_sweep(task, activation, optimizer_name, batch_sizes=[32,64,128,256], epochs=12, lr=None):\n",
        "    rows = []\n",
        "    for bs in batch_sizes:\n",
        "        print(f\"Batch sweep: {task} {activation} {optimizer_name} bs={bs}\")\n",
        "        train_loader, test_loader, _ = get_dataloaders(task=task, batch_size=bs)\n",
        "        model = SimpleANN(activation).to(DEVICE) if task==\"mnist\" else SmallCNN(activation).to(DEVICE)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        lr = lr if lr is not None else LR_DEFAULT.get(optimizer_name, 0.001)\n",
        "        optimizer = get_optimizer(optimizer_name, model.parameters(), lr)\n",
        "        scaler = GradScaler() if DEVICE.type==\"cuda\" else None\n",
        "        for epoch in range(1, epochs+1):\n",
        "            tl, ta, gn = train_one_epoch(model, train_loader, optimizer, loss_fn, scaler)\n",
        "            vl, va = evaluate(model, test_loader, loss_fn)\n",
        "            print(f\"  epoch {epoch}/{epochs} | train_acc {ta:.3f} val_acc {va:.3f}\")\n",
        "        rows.append({\"task\":task, \"activation\":activation, \"optimizer\":optimizer_name, \"batch_size\":bs, \"final_val_acc\":va})\n",
        "    out = Path(RESULTS_ROOT) / \"summary\" / f\"batch_sweep_{task}_{activation}_{optimizer_name}.csv\"\n",
        "    save_metrics_csv(rows, out)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "0GkdLxGqLegT",
        "outputId": "9234c2c6-b835-4ca3-e29e-ba2f91cc5914"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    print(\"DEVICE:\", DEVICE)\\n    print(f\"Running grid: epochs={EPOCHS}, batch_size={BATCH_SIZE}\")\\n    run_grid(tasks=(\"mnist\",\"cifar\"),\\n             activations=(\"sigmoid\",\"relu\",\"leakyrelu\",\"gelu\"),\\n             optimizers_list=(\"sgd\",\"adam\",\"adamw\"),\\n             epochs=EPOCHS,\\n             batch_size=BATCH_SIZE,\\n             results_root=RESULTS_ROOT)\\n    print(\"Done. Check the results/ folder for outputs and CSVs.\")\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"DEVICE:\", DEVICE)\n",
        "    print(f\"Running grid: epochs={EPOCHS}, batch_size={BATCH_SIZE}\")\n",
        "    run_grid(tasks=(\"mnist\",\"cifar\"),\n",
        "             activations=(\"sigmoid\",\"relu\",\"leakyrelu\",\"gelu\"),\n",
        "             optimizers_list=(\"sgd\",\"adam\",\"adamw\"),\n",
        "             epochs=EPOCHS,\n",
        "             batch_size=BATCH_SIZE,\n",
        "             results_root=RESULTS_ROOT)\n",
        "    print(\"Done. Check the results/ folder for outputs and CSVs.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}