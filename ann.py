# -*- coding: utf-8 -*-
"""ANN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ynZhtONALeKB1aaQw6BX-TPX5rHYQMO
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt


torch.manual_seed(42)


device = torch.device("cpu")

# MNIST Dataset
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# model definition
class SimpleANN(nn.Module):
    def __init__(self, activation="relu"):
        super(SimpleANN, self).__init__()
        activations = {
            "sigmoid": nn.Sigmoid(),
            "relu": nn.ReLU(),
            "leakyrelu": nn.LeakyReLU(0.01),
            "gelu": nn.GELU(),
        }
        act = activations[activation.lower()]
        self.layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 256), act,
            nn.Linear(256, 128), act,
            nn.Linear(128, 10)
        )
    def forward(self, x):
        return self.layers(x)

#Training & Evaluation
def train_one_epoch(model, loader, optimizer, loss_fn):
    model.train()
    total_loss, total_correct = 0, 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = loss_fn(out, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x.size(0)
        total_correct += (out.argmax(1) == y).sum().item()
    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)

def evaluate(model, loader, loss_fn):
    model.eval()
    total_loss, total_correct = 0, 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = loss_fn(out, y)
            total_loss += loss.item() * x.size(0)
            total_correct += (out.argmax(1) == y).sum().item()
    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)

# experiment running
def run_experiment(activation="relu", optimizer_name="sgd", epochs=5, lr=0.01):
    model = SimpleANN(activation).to(device)
    loss_fn = nn.CrossEntropyLoss()

    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=lr)
    elif optimizer_name == "adamw":
        optimizer = optim.AdamW(model.parameters(), lr=lr)
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSProp(model.parameters(), lr=lr)

    train_losses, val_losses, train_accs, val_accs = [], [], [], []

    for epoch in range(epochs):
        tl, ta = train_one_epoch(model, train_loader, optimizer, loss_fn)
        vl, va = evaluate(model, test_loader, loss_fn)
        train_losses.append(tl); train_accs.append(ta)
        val_losses.append(vl); val_accs.append(va)
        print(f"Epoch {epoch+1}/{epochs} | Train Acc: {ta:.3f} | Val Acc: {va:.3f}")

    return train_losses, val_losses, train_accs, val_accs

# ADAM + RELU
train_losses, val_losses, train_accs, val_accs = run_experiment("relu", "adam", epochs=5, lr=0.001)

#results plotting
plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy")
plt.legend(); plt.title("ReLU + Adam")
plt.show()

# Sigmoid + SGD
train_losses, val_losses, train_accs, val_accs = run_experiment("sigmoid", "sgd", epochs=5, lr=0.01)
#result s
plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy")
plt.legend(); plt.title("Sigmoid + SGD")
plt.show()

# Sigmoid + SGD
train_losses, val_losses, train_accs, val_accs = run_experiment("sigmoid", "sgd", epochs=5, lr=0.01)
#RESULTS
plt.plot(train_accs, label="Train Acc")
plt.plot(val_accs, label="Val Acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy")
plt.legend(); plt.title("Sigmoid + SGD")
plt.show()

## results fro comaprison
results = {}
for act in ["sigmoid", "relu", "leakyrelu", "gelu"]:
    for opt in ["sgd", "adam", "adamw"]:
        lr = 0.01 if opt == "sgd" else 0.001
        print(f"Running {act} + {opt} (lr={lr})")
        train_losses, val_losses, train_accs, val_accs = run_experiment(act, opt, epochs=5, lr=lr)
        results[f"{act}_{opt}"] = (train_accs, val_accs)

## plot comparison

plt.figure(figsize=(8,6))
for act in ["sigmoid", "relu", "leakyrelu", "gelu"]:
    _, val_accs = results[f"{act}_adam"]
    plt.plot(val_accs, label=act)
plt.xlabel("Epoch"); plt.ylabel("Validation Accuracy")
plt.title("Activation Functions with Adam")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
for act in ["sigmoid", "relu", "leakyrelu", "gelu"]:
    _, val_accs = results[f"{act}_adamw"]
    plt.plot(val_accs, label=act)
plt.xlabel("Epoch"); plt.ylabel("Validation Accuracy")
plt.title("Activation Functions with Adamw")
plt.legend()
plt.show()

plt.figure(figsize=(8,6))
for act in ["sigmoid", "relu", "leakyrelu", "gelu"]:
    _, val_accs = results[f"{act}_sgd"]
    plt.plot(val_accs, label=act)
plt.xlabel("Epoch"); plt.ylabel("Validation Accuracy")
plt.title("Activation Functions with sgd")
plt.legend()
plt.show()

## fixed ADAM optimizer
plt.figure(figsize=(8,6))
for act in ["sigmoid", "relu", "leakyrelu", "gelu"]:
    _, val_accs = results[f"{act}_adam"]
    plt.plot(range(1, len(val_accs)+1), val_accs, marker='o', label=act)

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy")
plt.title("Activation Functions (Optimizer = Adam)")
plt.legend()
plt.grid(True)
plt.show()

## fixed AF - RELU
plt.figure(figsize=(8,6))
for opt in ["sgd", "adam", "adamw"]:
    _, val_accs = results[f"relu_{opt}"]
    plt.plot(range(1, len(val_accs)+1), val_accs, marker='o', label=opt)

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy")
plt.title("Optimizers (Activation = ReLU)")
plt.legend()
plt.grid(True)
plt.show()

## gradient norm tracking inside train_one_epoch

def train_one_epoch(model, loader, optimizer, loss_fn):
    model.train()
    total_loss, total_correct = 0, 0
    grad_norms = []   # NEW: list to store gradient norms

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = loss_fn(out, y)
        loss.backward()

        # --- Gradient magnitude tracking ---
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2).item()  # L2 norm
                total_norm += param_norm ** 2
        total_norm = total_norm ** 0.5
        grad_norms.append(total_norm)

        optimizer.step()
        total_loss += loss.item() * x.size(0)
        total_correct += (out.argmax(1) == y).sum().item()

    avg_grad_norm = sum(grad_norms) / len(grad_norms)  # epoch-wise average
    return total_loss/len(loader.dataset), total_correct/len(loader.dataset), avg_grad_norm

def run_experiment(activation="relu", optimizer_name="sgd", epochs=5, lr=0.01):
    model = SimpleANN(activation).to(device)
    loss_fn = nn.CrossEntropyLoss()

    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=lr)
    elif optimizer_name == "adamw":
        optimizer = optim.AdamW(model.parameters(), lr=lr)

    train_losses, val_losses, train_accs, val_accs, grad_norms = [], [], [], [], []

    for epoch in range(epochs):
        tl, ta, gn = train_one_epoch(model, train_loader, optimizer, loss_fn)
        vl, va = evaluate(model, test_loader, loss_fn)
        train_losses.append(tl); train_accs.append(ta)
        val_losses.append(vl); val_accs.append(va); grad_norms.append(gn)
        print(f"Epoch {epoch+1}/{epochs} | Train Acc: {ta:.3f} | Val Acc: {va:.3f} | GradNorm: {gn:.4f}")

    return train_losses, val_losses, train_accs, val_accs, grad_norms

_, _, _, _, grad_sigmoid = run_experiment("sigmoid", "sgd", epochs=5, lr=0.01)
_, _, _, _, grad_relu = run_experiment("relu", "sgd", epochs=5, lr=0.01)

plt.figure(figsize=(8,6))
plt.plot(range(1, len(grad_sigmoid)+1), grad_sigmoid, marker='o', label="Sigmoid")
plt.plot(range(1, len(grad_relu)+1), grad_relu, marker='o', label="ReLU")
plt.xlabel("Epochs")
plt.ylabel("Average Gradient Norm")
plt.title("Gradient Flow: Sigmoid vs ReLU (SGD)")
plt.legend()
plt.grid(True)
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os

# --- Configuration ---
torch.manual_seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
RESULTS_DIR = "cnn_experiment_results"
os.makedirs(RESULTS_DIR, exist_ok=True)

# --- Data Loading (MNIST) ---
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# --- 1. CNN Model Definition ---
class SimpleCNN(nn.Module):
    def __init__(self, activation="relu"):
        super(SimpleCNN, self).__init__()

        # New Activations Dictionary including SiLU/Swish
        activations = {
            "sigmoid": nn.Sigmoid(),
            "relu": nn.ReLU(),
            "leakyrelu": nn.LeakyReLU(0.01),
            "gelu": nn.GELU(),
            "silu": nn.SiLU(), # Sigmoid Linear Unit (Swish)
        }
        act = activations[activation.lower()]

        self.features = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            act,
            nn.MaxPool2d(kernel_size=2, stride=2), # Output: 32x14x14
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            act,
            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 64x7x7
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            act,
            nn.Linear(128, 10)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# --- Training & Evaluation Functions ---

# Modified to include gradient norm tracking and evaluation
def train_one_epoch_grad(model, loader, optimizer, loss_fn):
    model.train()
    total_loss, total_correct = 0, 0
    grad_norms = []

    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = loss_fn(out, y)
        loss.backward()

        # Gradient magnitude tracking
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2).item()
                total_norm += param_norm ** 2
        total_norm = total_norm ** 0.5
        grad_norms.append(total_norm)

        optimizer.step()
        total_loss += loss.item() * x.size(0)
        total_correct += (out.argmax(1) == y).sum().item()

    avg_grad_norm = sum(grad_norms) / len(grad_norms)
    return total_loss/len(loader.dataset), total_correct/len(loader.dataset), avg_grad_norm

def evaluate(model, loader, loss_fn):
    model.eval()
    total_loss, total_correct = 0, 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            out = model(x)
            loss = loss_fn(out, y)
            total_loss += loss.item() * x.size(0)
            total_correct += (out.argmax(1) == y).sum().item()
    return total_loss/len(loader.dataset), total_correct/len(loader.dataset)

# --- 2. Experiment Runner (Updated for CNN and Optimizers) ---
def run_experiment(activation="relu", optimizer_name="sgdm", epochs=5, lr=0.001):
    # Use the CNN model
    model = SimpleCNN(activation).to(device)
    loss_fn = nn.CrossEntropyLoss()

    # Updated Optimizer Logic
    if optimizer_name == "sgdm":
        # SGDM (SGD with Momentum)
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=lr)
    elif optimizer_name == "adamw":
        optimizer = optim.AdamW(model.parameters(), lr=lr)
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=lr)

    train_losses, val_losses, train_accs, val_accs, grad_norms = [], [], [], [], []

    for epoch in range(epochs):
        tl, ta, gn = train_one_epoch_grad(model, train_loader, optimizer, loss_fn)
        vl, va = evaluate(model, test_loader, loss_fn)
        train_losses.append(tl); train_accs.append(ta)
        val_losses.append(vl); val_accs.append(va); grad_norms.append(gn)

        # Optional: Print epoch results
        # print(f"Epoch {epoch+1}/{epochs} | {activation}+{optimizer_name} | Val Acc: {va:.4f}")

    return train_losses, val_losses, train_accs, val_accs, grad_norms

# --- 3. Run and Plot Comparisons ---

print("\n--- Starting Comprehensive CNN Experiment ---")

ACTIVATIONS = ["sigmoid", "relu", "leakyrelu", "gelu", "silu"]
OPTIMIZERS = ["sgdm", "adam", "adamw", "rmsprop"]
EPOCHS = 5

results = {}

for act in ACTIVATIONS:
    for opt in OPTIMIZERS:
        # Lower LR for adaptive optimizers like Adam/RMSprop, higher for SGDM
        lr = 0.01 if opt == "sgdm" else 0.001

        print(f"Running {act} + {opt} (lr={lr})")

        train_l, val_l, train_a, val_a, grad_n = run_experiment(act, opt, epochs=EPOCHS, lr=lr)
        results[f"{act}_{opt}"] = (train_a, val_a, grad_n)


# --- 4. Plot Generation ---

# 4.1 Activation Functions Comparison (Fixed Optimizer: ADAMW)
plt.figure(figsize=(10, 6))
for act in ACTIVATIONS:
    _, val_accs, _ = results[f"{act}_adamw"]
    plt.plot(range(1, EPOCHS + 1), val_accs, marker='o', label=act.upper())

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy")
plt.title(f"Activation Functions with AdamW (CNN on MNIST)")
plt.legend(loc='lower right')
plt.grid(True)
plt.savefig(os.path.join(RESULTS_DIR, "af_comparison_adamw.png"))
plt.close()

# 4.2 Optimizer Comparison (Fixed Activation: SiLU)
plt.figure(figsize=(10, 6))
for opt in OPTIMIZERS:
    _, val_accs, _ = results[f"silu_{opt}"]
    plt.plot(range(1, EPOCHS + 1), val_accs, marker='o', label=opt.upper())

plt.xlabel("Epochs")
plt.ylabel("Validation Accuracy")
plt.title(f"Optimizers with SiLU (CNN on MNIST)")
plt.legend(loc='lower right')
plt.grid(True)
plt.savefig(os.path.join(RESULTS_DIR, "opt_comparison_silu.png"))
plt.close()

# 4.3 Gradient Flow Comparison (Fixed Optimizer: SGDM)
plt.figure(figsize=(10, 6))
for act in ACTIVATIONS:
    _, _, grad_norms = results[f"{act}_sgdm"]
    plt.plot(range(1, EPOCHS + 1), grad_norms, marker='o', label=act.upper())

plt.xlabel("Epochs")
plt.ylabel("Average Gradient Norm")
plt.title(f"Gradient Flow with SGDM (CNN on MNIST)")
plt.legend(loc='upper right')
plt.grid(True)
plt.savefig(os.path.join(RESULTS_DIR, "grad_flow_sgdm.png"))
plt.close()

print("\nAll CNN experiments complete. Results saved in 'cnn_experiment_results' directory.")
print("Generated plots: af_comparison_adamw.png, opt_comparison_silu.png, grad_flow_sgdm.png")

